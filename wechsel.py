# -*- coding: utf-8 -*-
"""Wechsel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lTRFkVOznRS4rkjL65NINT9L79hvXQF4

# Libraries
"""

!pip install accelerate -U

!pip install datasets

import numpy as np
from google.colab import drive
from datasets import load_metric
from transformers import Trainer
from datasets import load_dataset
from transformers import pipeline
from transformers import AutoTokenizer
from transformers import TrainingArguments
from transformers import AutoModelForTokenClassification
from transformers import AutoModelForSequenceClassification
from sklearn.metrics import precision_recall_fscore_support

drive.mount('/content/drive')

"""# Functions"""

def preprocess_function_for_NLI(examples, tokenizer):
    return tokenizer(examples['premise'], examples['hypothesis'], truncation = True, padding = 'max_length', max_length = 128)

metric_for_NLI = load_metric("accuracy")

def compute_metrics_for_NLI(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis = -1)
    return metric_for_NLI.compute(predictions = predictions, references = labels)

def preprocess_function_for_NER(examples, tokenizer):
  total_adjusted_labels = []
  tokenized_samples = tokenizer.batch_encode_plus(examples["tokens"], is_split_into_words = True, padding = 'max_length', truncation = True, max_length = 128)

  for k in range(0, len(tokenized_samples["input_ids"])):
    prev_wid = -1
    word_ids_list = tokenized_samples.word_ids(batch_index=k)
    existing_label_ids = examples["ner_tags"][k]

    i = -1
    adjusted_label_ids = []

    for wid in word_ids_list:
      if(wid is None):
        adjusted_label_ids.append(-100)
      elif(wid!=prev_wid):
        i = i + 1
        adjusted_label_ids.append(existing_label_ids[i])
        prev_wid = wid
      else:
        adjusted_label_ids.append(existing_label_ids[i])

    total_adjusted_labels.append(adjusted_label_ids)

  tokenized_samples["labels"] = total_adjusted_labels

  return tokenized_samples

metric_for_NER = load_metric("f1")

def compute_metrics_for_NER(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis = 2)

    true_predictions = [
        [p for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [l for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    true_predictions = [item for sublist in true_predictions for item in sublist]
    true_labels = [item for sublist in true_labels for item in sublist]

    result = metric_for_NER.compute(predictions = true_predictions, references = true_labels, average = "micro")

    return {
        "f1": result["f1"]
    }

"""# Models

## French WECHSEL-RoBERTa Model
"""

model_name = "benjamin/roberta-base-wechsel-french"

"""### NLI Task"""

xnli_fr = load_dataset("xnli", 'fr')

tokenizer_xnli_fr = AutoTokenizer.from_pretrained(model_name)
model_xnli_fr = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 3)

encoded_xnli_fr = xnli_fr.map(lambda examples: preprocess_function_for_NLI(examples, tokenizer_xnli_fr), batched = True)
train_xnli_fr = encoded_xnli_fr["train"].shuffle(seed=42).select(range(25000))

path_xnli_fr = '/content/drive/My Drive/NLI/french'

training_args_xnli_fr = TrainingArguments(
    output_dir=path_xnli_fr,
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=128,
    per_device_eval_batch_size=128,
    num_train_epochs=2,
    weight_decay=0.01,
    logging_steps=196,
    save_strategy="epoch",
    save_total_limit=1,
    adam_epsilon=1e-8,
    adam_beta1=0.9,
    adam_beta2=0.999,
    lr_scheduler_type='linear',
    warmup_ratio=0.1
)

trainer_xnli_fr = Trainer(
    model = model_xnli_fr,
    args = training_args_xnli_fr,
    train_dataset = train_xnli_fr,
    eval_dataset = encoded_xnli_fr["validation"],
    compute_metrics = compute_metrics_for_NLI,
)

trainer_xnli_fr.train()

"""### NER Task"""

wikiann_fr = load_dataset("wikiann", "fr")

tokenizer_wikiann_fr = AutoTokenizer.from_pretrained(model_name, add_prefix_space = True)
model_wikiann_fr = AutoModelForTokenClassification.from_pretrained(model_name, num_labels = 7)

encoded_wikiann_fr = wikiann_fr.map(lambda examples: preprocess_function_for_NER(examples, tokenizer_wikiann_fr), batched = True)

path_wikiann_fr = '/content/drive/My Drive/NER/french'

training_args_wikiann_fr = TrainingArguments(
    output_dir = path_wikiann_fr,
    evaluation_strategy = "epoch",
    learning_rate = 2e-5,
    per_device_train_batch_size = 32,
    per_device_eval_batch_size = 32,
    num_train_epochs = 10,
    weight_decay = 0.01,
    logging_steps = 3125,
    save_strategy = "epoch",
    save_total_limit = 1,
    adam_epsilon = 1e-8,
    adam_beta1 = 0.9,
    adam_beta2 = 0.999,
    lr_scheduler_type = 'linear',
    warmup_ratio = 0.1
)

trainer_wikiann_fr = Trainer(
    model = model_wikiann_fr,
    args = training_args_wikiann_fr,
    train_dataset = encoded_wikiann_fr["train"],
    eval_dataset = encoded_wikiann_fr["validation"],
    compute_metrics = compute_metrics_for_NER
)

trainer_wikiann_fr.train()

"""## German WECHSEL-RoBERTa Model"""

model_name = "benjamin/roberta-base-wechsel-german"

"""### NLI Task"""

xnli_de = load_dataset("xnli", 'de')

tokenizer_xnli_de = AutoTokenizer.from_pretrained(model_name)
model_xnli_de = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 3)

encoded_xnli_de = xnli_de.map(lambda examples: preprocess_function_for_NLI(examples, tokenizer_xnli_de), batched = True)
train_xnli_de = encoded_xnli_de["train"].shuffle(seed=42).select(range(25000))

path_xnli_de = '/content/drive/My Drive/NLI/german'

training_args_xnli_de = TrainingArguments(
    output_dir=path_xnli_de,
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=128,
    per_device_eval_batch_size=128,
    num_train_epochs=2,
    weight_decay=0.01,
    logging_steps=196,
    save_strategy="epoch",
    save_total_limit=1,
    adam_epsilon=1e-8,
    adam_beta1=0.9,
    adam_beta2=0.999,
    lr_scheduler_type='linear',
    warmup_ratio=0.1
)

trainer_xnli_de = Trainer(
    model = model_xnli_de,
    args = training_args_xnli_de,
    train_dataset = train_xnli_de,
    eval_dataset = encoded_xnli_de["validation"],
    compute_metrics = compute_metrics_for_NLI,
)

trainer_xnli_de.train()

"""### NER Task"""

wikiann_de = load_dataset("wikiann", "de")

tokenizer_wikiann_de = AutoTokenizer.from_pretrained(model_name, add_prefix_space = True)
model_wikiann_de = AutoModelForTokenClassification.from_pretrained(model_name, num_labels = 7)

encoded_wikiann_de = wikiann_de.map(lambda examples: preprocess_function_for_NER(examples, tokenizer_wikiann_de), batched = True)

path_wikiann_de = '/content/drive/My Drive/NER/german'

training_args_wikiann_de = TrainingArguments(
    output_dir = path_wikiann_de,
    evaluation_strategy = "epoch",
    learning_rate = 2e-5,
    per_device_train_batch_size = 32,
    per_device_eval_batch_size = 32,
    num_train_epochs = 10,
    weight_decay = 0.01,
    logging_steps = 3125,
    save_strategy = "epoch",
    save_total_limit = 1,
    adam_epsilon = 1e-8,
    adam_beta1 = 0.9,
    adam_beta2 = 0.999,
    lr_scheduler_type = 'linear',
    warmup_ratio = 0.1
)

trainer_wikiann_de = Trainer(
    model = model_wikiann_de,
    args = training_args_wikiann_de,
    train_dataset = encoded_wikiann_de["train"],
    eval_dataset = encoded_wikiann_de["validation"],
    compute_metrics = compute_metrics_for_NER
)

trainer_wikiann_de.train()

"""## Chinese WECHSEL-RoBERTa Model"""

model_name = "benjamin/roberta-base-wechsel-chinese"

"""### NLI Task"""

xnli_zh = load_dataset("xnli", 'zh')

tokenizer_xnli_zh = AutoTokenizer.from_pretrained(model_name)
model_xnli_zh = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 3)

encoded_xnli_zh = xnli_zh.map(lambda examples: preprocess_function_for_NLI(examples, tokenizer_xnli_zh), batched = True)
train_xnli_zh = encoded_xnli_zh["train"].shuffle(seed=42).select(range(25000))

path_xnli_zh = '/content/drive/My Drive/NLI/chinese'

training_args_xnli_zh = TrainingArguments(
    output_dir=path_xnli_zh,
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=128,
    per_device_eval_batch_size=128,
    num_train_epochs=2,
    weight_decay=0.01,
    logging_steps=196,
    save_strategy="epoch",
    save_total_limit=1,
    adam_epsilon=1e-8,
    adam_beta1=0.9,
    adam_beta2=0.999,
    lr_scheduler_type='linear',
    warmup_ratio=0.1
)

trainer_xnli_zh = Trainer(
    model = model_xnli_zh,
    args = training_args_xnli_zh,
    train_dataset = train_xnli_zh,
    eval_dataset = encoded_xnli_zh["validation"],
    compute_metrics = compute_metrics_for_NLI,
)

trainer_xnli_zh.train()

"""### NER Task"""

wikiann_zh = load_dataset("wikiann", "zh")

tokenizer_wikiann_zh = AutoTokenizer.from_pretrained(model_name, add_prefix_space = True)
model_wikiann_zh = AutoModelForTokenClassification.from_pretrained(model_name, num_labels = 7)

encoded_wikiann_zh = wikiann_zh.map(lambda examples: preprocess_function_for_NER(examples, tokenizer_wikiann_zh), batched = True)

path_wikiann_zh = '/content/drive/My Drive/NER/chinese'

training_args_wikiann_zh = TrainingArguments(
    output_dir = path_wikiann_zh,
    evaluation_strategy = "epoch",
    learning_rate = 2e-5,
    per_device_train_batch_size = 32,
    per_device_eval_batch_size = 32,
    num_train_epochs = 10,
    weight_decay = 0.01,
    logging_steps = 3125,
    save_strategy = "epoch",
    save_total_limit = 1,
    adam_epsilon = 1e-8,
    adam_beta1 = 0.9,
    adam_beta2 = 0.999,
    lr_scheduler_type = 'linear',
    warmup_ratio = 0.1
)

trainer_wikiann_zh = Trainer(
    model = model_wikiann_zh,
    args = training_args_wikiann_zh,
    train_dataset = encoded_wikiann_zh["train"],
    eval_dataset = encoded_wikiann_zh["validation"],
    compute_metrics = compute_metrics_for_NER
)

trainer_wikiann_zh.train()

"""## Swahili WECHSEL-RoBERTa Model"""

model_name = "benjamin/roberta-base-wechsel-swahili"

"""### NLI Task"""

xnli_sw = load_dataset("xnli", 'sw')

tokenizer_xnli_sw = AutoTokenizer.from_pretrained(model_name)
model_xnli_sw = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 3)

encoded_xnli_sw = xnli_sw.map(lambda examples: preprocess_function_for_NLI(examples, tokenizer_xnli_sw), batched = True)
train_xnli_sw = encoded_xnli_sw["train"].shuffle(seed=42).select(range(25000))

path_xnli_sw = '/content/drive/My Drive/NLI/swahili'

training_args_xnli_sw = TrainingArguments(
    output_dir=path_xnli_sw,
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=128,
    per_device_eval_batch_size=128,
    num_train_epochs=2,
    weight_decay=0.01,
    logging_steps=196,
    save_strategy="epoch",
    save_total_limit=1,
    adam_epsilon=1e-8,
    adam_beta1=0.9,
    adam_beta2=0.999,
    lr_scheduler_type='linear',
    warmup_ratio=0.1
)

trainer_xnli_sw = Trainer(
    model = model_xnli_sw,
    args = training_args_xnli_sw,
    train_dataset = train_xnli_sw,
    eval_dataset = encoded_xnli_sw["validation"],
    compute_metrics = compute_metrics_for_NLI,
)

trainer_xnli_sw.train()

"""# Borrador

### NER Task
"""

wikiann_sw = load_dataset("wikiann", "sw")

tokenizer_wikiann_sw = AutoTokenizer.from_pretrained(model_name, add_prefix_space = True)
model_wikiann_sw = AutoModelForTokenClassification.from_pretrained(model_name, num_labels = 7)

encoded_wikiann_sw = wikiann_sw.map(lambda examples: preprocess_function_for_NER(examples, tokenizer_wikiann_sw), batched = True)

path_wikiann_sw = '/content/drive/My Drive/NER/swahili'

training_args_wikiann_sw = TrainingArguments(
    output_dir = path_wikiann_sw,
    evaluation_strategy = "epoch",
    learning_rate = 2e-5,
    per_device_train_batch_size = 32,
    per_device_eval_batch_size = 32,
    num_train_epochs = 10,
    weight_decay = 0.01,
    logging_steps = 3125,
    save_strategy = "epoch",
    save_total_limit = 1,
    adam_epsilon = 1e-8,
    adam_beta1 = 0.9,
    adam_beta2 = 0.999,
    lr_scheduler_type = 'linear',
    warmup_ratio = 0.1
)

trainer_wikiann_sw = Trainer(
    model = model_wikiann_sw,
    args = training_args_wikiann_sw,
    train_dataset = encoded_wikiann_sw["train"],
    eval_dataset = encoded_wikiann_sw["validation"],
    compute_metrics = compute_metrics_for_NER
)

trainer_wikiann_sw.train()

"""### German WECHSEL-GPT2 Model"""

wikiann_de = load_dataset("wikiann", "de")

model_name = "benjamin/gpt2-wechsel-german"
tokenizer_wikiann_de = AutoTokenizer.from_pretrained(model_name, add_prefix_space = True)
model_wikiann_de = AutoModelForTokenClassification.from_pretrained(model_name, num_labels = 7)

tokenizer_wikiann_de.add_special_tokens({'pad_token': '[PAD]'})
model_wikiann_de.resize_token_embeddings(len(tokenizer_wikiann_de))

encoded_wikiann_de = wikiann_de.map(lambda examples: preprocess_function(examples, tokenizer_wikiann_de), batched = True)

path_wikiann_de = '/content/drive/My Drive/NER/german'

training_args_wikiann_de = TrainingArguments(
    output_dir = path_wikiann_de,
    evaluation_strategy = "epoch",
    learning_rate = 2e-5,
    per_device_train_batch_size = 32,
    per_device_eval_batch_size = 32,
    num_train_epochs = 10,
    weight_decay = 0.01,
    logging_steps = 1250,
    save_strategy = "epoch",
    save_total_limit = 1,
    adam_epsilon = 1e-8,
    adam_beta1 = 0.9,
    adam_beta2 = 0.999,
    lr_scheduler_type = 'linear',
    warmup_ratio = 0.1
)

trainer_wikiann_de = Trainer(
    model = model_wikiann_de,
    args = training_args_wikiann_de,
    train_dataset = encoded_wikiann_de["train"],
    eval_dataset = encoded_wikiann_de["validation"],
    compute_metrics = compute_metrics
)

trainer_wikiann_de.train()

"""### Chinese WECHSEL-GPT2 Model"""

wikiann_zh = load_dataset("wikiann", "zh")

model_name = "benjamin/gpt2-wechsel-chinese"
tokenizer_wikiann_zh = AutoTokenizer.from_pretrained(model_name, add_prefix_space = True)
model_wikiann_zh = AutoModelForTokenClassification.from_pretrained(model_name, num_labels = 7)

tokenizer_wikiann_zh.add_special_tokens({'pad_token': '[PAD]'})
model_wikiann_zh.resize_token_embeddings(len(tokenizer_wikiann_zh))

encoded_wikiann_zh = wikiann_zh.map(lambda examples: preprocess_function(examples, tokenizer_wikiann_zh), batched = True)

path_wikiann_zh = '/content/drive/My Drive/NER/chinese'

training_args_wikiann_zh = TrainingArguments(
    output_dir = path_wikiann_zh,
    evaluation_strategy = "epoch",
    learning_rate = 2e-5,
    per_device_train_batch_size = 32,
    per_device_eval_batch_size = 32,
    num_train_epochs = 10,
    weight_decay = 0.01,
    logging_steps = 1250,
    save_strategy = "epoch",
    save_total_limit = 1,
    adam_epsilon = 1e-8,
    adam_beta1 = 0.9,
    adam_beta2 = 0.999,
    lr_scheduler_type = 'linear',
    warmup_ratio = 0.1
)

trainer_wikiann_zh = Trainer(
    model = model_wikiann_zh,
    args = training_args_wikiann_zh,
    train_dataset = encoded_wikiann_zh["train"],
    eval_dataset = encoded_wikiann_zh["validation"],
    compute_metrics = compute_metrics
)

trainer_wikiann_zh.train()

"""### Swahili WECHSEL-GPT2 Model"""

wikiann_sw = load_dataset("wikiann", "sw")

model_name = "benjamin/gpt2-wechsel-swahili"
tokenizer_wikiann_sw = AutoTokenizer.from_pretrained(model_name, add_prefix_space = True)
model_wikiann_sw = AutoModelForTokenClassification.from_pretrained(model_name, num_labels = 7)

tokenizer_wikiann_sw.add_special_tokens({'pad_token': '[PAD]'})
model_wikiann_sw.resize_token_embeddings(len(tokenizer_wikiann_sw))

encoded_wikiann_sw = wikiann_sw.map(lambda examples: preprocess_function(examples, tokenizer_wikiann_sw), batched = True)

path_wikiann_sw = '/content/drive/My Drive/NER/swahili'

training_args_wikiann_sw = TrainingArguments(
    output_dir = path_wikiann_sw,
    evaluation_strategy = "epoch",
    learning_rate = 2e-5,
    per_device_train_batch_size = 32,
    per_device_eval_batch_size = 32,
    num_train_epochs = 10,
    weight_decay = 0.01,
    logging_steps = 1250,
    save_strategy = "epoch",
    save_total_limit = 1,
    adam_epsilon = 1e-8,
    adam_beta1 = 0.9,
    adam_beta2 = 0.999,
    lr_scheduler_type = 'linear',
    warmup_ratio = 0.1
)

trainer_wikiann_sw = Trainer(
    model = model_wikiann_sw,
    args = training_args_wikiann_sw,
    train_dataset = encoded_wikiann_sw["train"],
    eval_dataset = encoded_wikiann_sw["validation"],
    compute_metrics = compute_metrics
)

trainer_wikiann_sw.train()

model_name = "benjamin/gpt2-wechsel-french"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=9)

# Cargar el pipeline para NER
ner_pipeline = pipeline("ner", model=model, tokenizer=tokenizer, aggregation_strategy="simple")

# Cargar el conjunto de datos WikiANN para francés
dataset = load_dataset("wikiann", "fr")

# Mostrar un ejemplo del conjunto de datos
example = dataset["train"][0]
print(example)

# Definir una función para aplicar el pipeline de NER a una muestra del conjunto de datos
def apply_ner(text):
    return ner_pipeline(text)

# Probar con un ejemplo
text = "Paris est la capitale de la France."
ner_results = apply_ner(text)

for entity in ner_results:
    print(f"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']}")

from transformers import AutoTokenizer
from datasets import load_dataset, Dataset

# Definir una muestra de datos
examples = {
    "tokens": [["Hugging", "Face", "est", "une", "entreprise", "de", "technologie", "basée", "à", "New", "York"]],
    "ner_tags": [[1, 1, 0, 0, 0, 0, 0, 0, 0, 3, 3]]  # Ejemplo de etiquetas NER
}

# Crear un dataset de ejemplo
dataset = Dataset.from_dict(examples)

# Especificar el identificador del modelo
model_name = "benjamin/gpt2-wechsel-french"

# Cargar el tokenizador automáticamente
tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space = True)

# Definir la función de preprocesamiento
def preprocess_function(examples, tokenizer):
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)
    print(tokenized_inputs)
    labels = []
    for i, label in enumerate(examples["ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word
        print(word_ids)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)
            elif word_idx != previous_word_idx:
                label_ids.append(label[word_idx])
            else:
                label_ids.append(-100)
            previous_word_idx = word_idx
        labels.append(label_ids)
    tokenized_inputs["labels"] = labels
    return tokenized_inputs

# Aplicar la función de preprocesamiento
encoded_dataset = dataset.map(lambda examples: preprocess_function(examples, tokenizer), batched=True)

# Mostrar el resultado del preprocesamiento
for i in range(len(encoded_dataset)):
    print(f"Tokens: {encoded_dataset[i]['input_ids']}")
    print(f"Labels: {encoded_dataset[i]['labels']}")

for i in range(len(encoded_dataset[0]['input_ids'])):
    print(tokenizer.convert_ids_to_tokens(encoded_dataset[0]['input_ids'][i]))

from transformers import RobertaTokenizer, RobertaForSequenceClassification, pipeline
from transformers import Trainer, TrainingArguments
from tqdm import tqdm
import numpy as np
import torch

# Cargar el tokenizer y el modelo
model_name = "benjamin/roberta-base-wechsel-french"
tokenizer = RobertaTokenizer.from_pretrained(model_name)
model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels = 3)

# Definir las métricas de evaluación
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    accuracy = np.sum(predictions == labels) / labels.shape[0]
    return {"accuracy": accuracy}

# Configurar los argumentos del entrenamiento
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
)

# Crear el Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset["validation"],
    compute_metrics=compute_metrics,
)

# Entrenar el modelo
trainer.train()

# Evaluar el modelo
eval_results = trainer.evaluate()

print(f"Evaluation results: {eval_results}")

def predict(premise, hypothesis):
    inputs = tokenizer(premise, hypothesis, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1).item()
    return predictions

label_mapping = ["contradiction", "entailment", "neutral"]

correct_predictions = 0
total_predictions = 0

for sample in tqdm(xnli_dataset['test']):
    premise = sample['premise']
    hypothesis = sample['hypothesis']
    actual_label = sample['label']

    predicted_label = predict(premise, hypothesis)

    if predicted_label == actual_label:
        correct_predictions += 1
    total_predictions += 1

# Calcular la precisión (accuracy)
accuracy = correct_predictions / total_predictions

print(correct_predictions)
print(total_predictions)
print(f"Accuracy: {accuracy:.4f}")

print(model)

sample = xnli_dataset['test'][0]
premise = sample['premise']
hypothesis = sample['hypothesis']
label = sample['label']

# Tokenizar las premisas e hipótesis
inputs = tokenizer(premise, hypothesis, return_tensors="pt", padding=True, truncation=True)

# Hacer predicción con el modelo
with torch.no_grad():
    outputs = model(**inputs)

# Obtener las predicciones
logits = outputs.logits
predictions = torch.argmax(logits, dim=-1).item()

print(logits)
print(label)

# Mapear la predicción a la etiqueta correspondiente
label_mapping = ["contradiction", "entailment", "neutral"]
predicted_label = label_mapping[predictions]

# Imprimir los resultados
print(f"Premise: {premise}")
print(f"Hypothesis: {hypothesis}")
print(f"Predicted Label: {predicted_label}")
print(f"Actual Label: {label_mapping[label]}")

import os
from huggingface_hub import hf_hub_download

HUGGING_FACE_API_TOKEN = os.environ.get("HUGGING_FACE_API_TOKEN")

model_id = "benjamin/roberta-base-wechsel-french"
filenames = [
    "config.json", "merges.txt", "model.safetensors", "optimizer.pt",
    "pytorch_model.bin", "scheduler.pt", "special_tokens_map.json", "tokenizer.json",
    "tokenizer_config.json", "trainer_state.json", "training_args.bin", "vocab.json"
]

for filename in filenames:
    downloaded_model_path = hf_hub_download(
        repo_id = model_id,
        filename = filename,
        token = HUGGING_FACE_API_TOKEN
    )
    print(downloaded_model_path)

from transformers import AutoTokenizer, AutoModelForMaskedLM

tokenizer = AutoTokenizer.from_pretrained(model_id, legacy = False)
model = AutoModelForMaskedLM.from_pretrained(model_id)

from transformers import pipeline

m = pipeline("fill-mask", model="benjamin/roberta-base-wechsel-french")

print(model)

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(model_id, legacy = False)
model = AutoModelForCausalLM.from_pretrained(model_id)

from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer
from datasets import load_dataset
import torch

# Cargar el modelo y el tokenizador
model_name = "joeddav/xlm-roberta-large-xnli"
model = XLMRobertaForSequenceClassification.from_pretrained(model_name)
tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)

# Cargar el conjunto de datos XNLI
dataset = load_dataset("xnli", "es")  # Cargar la versión en español

# Preparar los datos
def preprocess_function(examples):
    # return tokenizer(examples['premise'], examples['hypothesis'], truncation=True)
    return tokenizer(examples['premise'], examples['hypothesis'], truncation=True, padding=True)

tokenized_dataset = dataset.map(preprocess_function, batched=True)

# Definir función para realizar inferencia
def predict(premise, hypothesis):
    inputs = tokenizer(premise, hypothesis, return_tensors='pt', truncation=True, padding=True)
    with torch.no_grad():
        logits = model(**inputs).logits
    probabilities = torch.softmax(logits, dim=-1).squeeze().tolist()
    labels = ["entailment", "neutral", "contradiction"]
    max_index = probabilities.index(max(probabilities))
    return labels[max_index], probabilities

# Prueba con una premisa e hipótesis
premise = "El gato está en la alfombra."
hypothesis = "Hay un animal en la alfombra."
label, probabilities = predict(premise, hypothesis)
print(f"Label: {label}, Probabilities: {probabilities}")

# Preparar los datos
def preprocess_function(examples):
    # return tokenizer(examples['premise'], examples['hypothesis'], truncation=True)
    return tokenizer(examples['premise'], examples['hypothesis'], truncation=True, padding=True)

tokenized_dataset = dataset.map(preprocess_function, batched=True)

# Definir función para realizar inferencia
def predict(premise, hypothesis):
    inputs = tokenizer(premise, hypothesis, return_tensors='pt', truncation=True, padding=True)
    with torch.no_grad():
        logits = model(**inputs).logits
    probabilities = torch.softmax(logits, dim=-1).squeeze().tolist()
    labels = ["entailment", "neutral", "contradiction"]
    max_index = probabilities.index(max(probabilities))
    return labels[max_index], probabilities

# Definir función para realizar inferencia
def predict(premise, hypothesis):
    inputs = tokenizer(premise, hypothesis, return_tensors='pt', truncation=True)
    with torch.no_grad():
        logits = model(**inputs).logits
    probabilities = torch.softmax(logits, dim=-1).squeeze().tolist()
    labels = ["contradiction", "neutral", "entailment"]
    max_index = probabilities.index(max(probabilities))
    return labels[max_index], probabilities

# Prueba con una premisa e hipótesis
premise = "El gato está sentado en la alfombra."
hypothesis = "Hay un animal en la alfombra."
label, probabilities = predict(premise, hypothesis)
print(f"Label: {label}, Probabilities: {probabilities}")

# Evaluar en el conjunto de datos de prueba
def compute_accuracy(model, dataset):
    correct = 0
    total = 0
    for example in dataset['test']:
        premise = example['premise']
        hypothesis = example['hypothesis']
        true_label = example['label']
        predicted_label, _ = predict(premise, hypothesis)
        if predicted_label == dataset['train'].features['label'].int2str(true_label):
            correct += 1
        total += 1
    return correct / total

accuracy = compute_accuracy(model, tokenized_dataset)
print(f"Accuracy on the test set: {accuracy:.2f}")